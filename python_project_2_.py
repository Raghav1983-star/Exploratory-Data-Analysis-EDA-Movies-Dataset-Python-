# -*- coding: utf-8 -*-
"""Python Project 2 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d5OFQ7LCmcsAIXpkSyN_15CVaUZ5cn6W
"""

# Import the numpy and pandas packages

import numpy as np
import pandas as pd

"""Task 1: Reading and Inspection

Subtask 1.1: Import and read

#Import and read the movie database. Store it in a variable called movies.
"""

import pandas as pd
import numpy as np

movies = pd.read_csv('/content/Movie+Assignment+Data.csv')
         # Write your code for importing the csv file here
movies

movies.head()

"""##Subtask 1.2: Inspect the dataframe

Inspect the dataframe's columns, shapes, variable types etc.
"""

[movies.columns]

[movies.shape]

[movies.info]

"""##Task 2: Cleaning the Data

##Subtask 2.1: Inspect Null values

Find out the number of Null values in all the columns and rows. Also, find the percentage of Null values in each column. Round off the percentages upto two decimal places.
"""

movies = pd.read_csv('/content/Movie+Assignment+Data.csv')
movies.isnull().sum ()# Write your code for column-wise null count here

[movies.isnull().sum (axis = 1)]# Write your code for row-wise null count here

# The (axis) parameter within the sum() function is used to change the direction
# of columns to rows.

import pandas as pd
movies = pd.read_csv('/content/Movie+Assignment+Data.csv')# Data loading
null_percentages = movies.isnull().mean() * 100
# Write your code for column-wise null percentages
print(null_percentages.round(2))

"""##Subtask 2.2: Drop unecessary columns

For this assignment, you will mostly be analyzing the movies with respect to the ratings, gross collection, popularity of movies, etc. So many of the columns in this dataframe are not required. So it is advised to drop the following columns.

color

director_facebook_likes

actor_1_facebook_likes

actor_2_facebook_likes

actor_3_facebook_likes

actor_2_name

cast_total_facebook_likes

actor_3_name

duration

facenumber_in_poster

content_rating

country

movie_imdb_link

aspect_ratio0

plot_keywords
"""

import pandas as pd
movies = pd.read_csv('//content/Movie+Assignment+Data.csv')
# Check if 'aspect_ratio0' is present in the column names, likely it is aspect_ratio
if 'aspect_ratio0' not in movies.columns:
    # If not present, replace with correct column name 'aspect_ratio'
    movies_dropped_columns = movies.drop(columns=['color', 'director_facebook_likes', 'actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',
                                                 'cast_total_facebook_likes', 'actor_3_name', 'duration', 'facenumber_in_poster', 'content_rating', 'country',
                                                 'movie_imdb_link', 'aspect_ratio', 'plot_keywords'])
else:
    # If it is present then proceed with original code:
    movies_dropped_columns = movies.drop(columns=['color', 'director_facebook_likes', 'actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',
                                                 'cast_total_facebook_likes', 'actor_3_name', 'duration', 'facenumber_in_poster', 'content_rating', 'country',
                                                 'movie_imdb_link', 'aspect_ratio0', 'plot_keywords'])
# Write your code for dropping the columns here.

"""##Subtask 2.3: Drop unecessary rows using columns with high Null percentages

Now, on inspection you might notice that some columns have large percentage (greater than 5%) of Null values. Drop all the rows which have Null values for such columns.
"""

import pandas as pd

movies = pd.read_csv('/content/Movie+Assignment+Data.csv')

# Calculate the percentage of null values in each column
null_percentages = movies.isnull().sum() / len(movies) * 100

# Identify columns with more than 5% null values
columns_to_drop_rows = null_percentages[null_percentages > 5].index

# Drop rows with null values in the identified columns
movies_cleaned = movies.dropna(subset=columns_to_drop_rows)

# Display the new shape of the DataFrame
print(movies_cleaned.shape)

"""##Subtask 2.4: Drop unecessary rows

Some of the rows might have greater than five NaN values. Such rows aren't of much use for the analysis and hence, should be removed.
"""

import pandas as pd

# Load the dataset
df = pd.read_csv("Movie+Assignment+Data.csv")

null_percentage =movies.isnull().mean()*100
print(null_percentage)

columns_with_high_nulls = null_percentage[null_percentage > 5].index
columns_with_high_nulls

movies_dropped_unnecessary_rows = movies.dropna(subset=columns_with_high_nulls)

print(movies_dropped_unnecessary_rows)
print(movies_dropped_unnecessary_rows.shape)

"""##Subtask 2.5: Fill NaN values

### "You might notice that the language column has some NaN values. Here, on inspection, you will see that it is safe to replace all the missing values with 'English'.
---
"""

df_cleaned.loc[:, 'language'] = df_cleaned['language'].fillna('English')
 #Write your code for filling the NaN values in the 'language' column here

"""Subtask 2.6: Check the number of retained rows

You might notice that two of the columns viz. num_critic_for_reviews and actor_1_name have small percentages of NaN values left. You can let these columns as it is for now. Check the number and percentage of the rows retained after completing all the tasks above.
"""

# Get the number of rows retained
num_rows_retained = df_cleaned.shape[0]

# Calculate the percentage of rows retained
percentage_retained = (num_rows_retained / df.shape[0]) * 100

# Display the results
print(f"Number of rows retained: {num_rows_retained}")
print(f"Percentage of rows retained: {percentage_retained:.2f}%")

"""Checkpoint 1: You might have noticed that we still have around 77% of the rows."

After Dropping unecessary rows using columns with high Null percentages we got **(3784 Rows and 28 columns)**


Total No. of Rows were **(5043)**
 Hence this accounted up to **75.03%**

**less than 77%**

##Task 3: Data Analysis

##Subtask 3.1: Change the unit of columns

Convert the unit of the budget and gross columns from    $  to   milllion  dollar.
"""

budget_million = movies['budget']/1000000 # Write your code for unit conversion here
                                     # Access the 'budget' column from the 'movies' DataFrame.
print(budget_million)

"""##Subtask 3.2: Find the movies with highest profit

1: Create a new column called profit which contains the difference of the two columns: gross and budget.

2: Sort the dataframe using the profit column as reference.

3: Extract the top ten profiting movies in descending order and store them in a new dataframe - top10"
"""

movies_cleaned['profit'] = movies_cleaned['gross'] - movies_cleaned['budget']
# Write your code for creating the profit column here

#Explaination
# We created a new column 'profit' by subtracting the'budget' column from the
# 'gross' column.
# profit=grossâˆ’budget
print(movies_cleaned['profit'])

# 2: Sort the DataFrame in descending order based on the 'profit' column
movies_sorted = movies_cleaned.sort_values(by='profit', ascending= False)
# Write your code for sorting the dataframe here


# Display the first few rows to verify sorting
print(movies_sorted.head())

[] #3:  Write your code to get the top 10 profiting movies here
top_10_profiting_movies = movies_cleaned.sort_values(by='profit', ascending=False).head(10)
print(top_10_profiting_movies)

"""##Subtask 3.3: Drop duplicate values

After you found out the top 10 profiting movies, you might have notice a duplicate value. So, it seems like the dataframe has duplicate values as well. Drop the duplicate values from the dataframe and repeat Subtask 3.2.
"""

[] # Remove duplicate rows from the dataset
df_cleaned_deduplicated = df_cleaned.drop_duplicates()

# Store the initial number of rows
initial_rows = df_cleaned.shape[0]  # Get the initial number of rows from df_cleaned

# Number of rows retained after removing duplicates
final_retained_rows = df_cleaned_deduplicated.shape[0]
final_retained_percentage = (final_retained_rows / initial_rows) * 100

# Display results
print(f"Rows retained after removing duplicates: {final_retained_rows}")
print(f"Percentage of rows retained: {final_retained_percentage:.2f}%")# Write your code for dropping duplicate values here

"""# Write code for repeating subtask 2 here\n"

1:Create a new column called profit which contains the difference of the two columns: gross and budget.

2: Sort the dataframe using the profit column as reference.

3: Extract the top ten profiting movies in descending order and store them in a new dataframe - top10"
"""

#1: Create a new column called profit which contains the difference of the two columns: gross and budget.
# Assuming your DataFrame is called 'movies_cleaned_deduplicated'

# We have ensure df_cleaned_deduplicated is defined (from the previous cell)
df_cleaned_deduplicated = df_cleaned.drop_duplicates().copy()  # Create an explicit copy using .copy()

# We have used .loc for assignment to avoid SettingWithCopyWarning
df_cleaned_deduplicated.loc[:, 'profit'] = df_cleaned_deduplicated['gross'] - df_cleaned_deduplicated['budget']# Write your code for dropping duplicate values here
print(df_cleaned_deduplicated.loc[:, 'profit'])

#2: Sort the dataframe using the profit column as reference.

# Sort the DataFrame in descending order based on the 'profit' column
df_cleaned_deduplicated = df_cleaned_deduplicated.sort_values(by='profit', ascending=False)
print(df_cleaned_deduplicated)

#3:Extract the top ten profiting movies in descending order and store them in a new dataframe - top10"

# Extract the top 10 profiting movies
top10_cleaned_deduplicated= df_cleaned_deduplicated.head(10)
print(top10_cleaned_deduplicated)

"""Checkpoint 2: You might spot two movies directed by James Cameron in the list.

Answer :Yes there are two movies directed by James Cameron.
  0 =  Action|Adventure|Fantasy|
        Sci-Fi

 26 = Drama|Romance

##Subtask 3.4: Find IMDb Top 250

1: Create a new dataframe IMDb_Top_250 and store the top 250 movies with the highest IMDb Rating (corresponding to the column: imdb_score). Also make sure that for all of these movies, the num_voted_users is greater than 25,000. Also add a Rank column containing the values 1 to 250 indicating the ranks of the corresponding films.

2:. Extract all the movies in the IMDb_Top_250 dataframe which are not in the English language and store them in a new dataframe named `Top_Foreign_Lang_Film
"""

#1: Write your code for extracting the top 250 movies as per the IMDb score here. Make sure that you store it in a new dataframe
# and name that dataframe as 'IMDb_Top_250'
top_250_movies = movies_cleaned.sort_values(by='imdb_score', ascending=False).head(250) #Fixed the indentation and applied .head(250) properly
top_250_movies = top_250_movies[top_250_movies['num_voted_users'] > 25000]
top_250_movies['Rank'] = range(1,len(top_250_movies) +1)
IMDb_Top_250 = top_250_movies
print(IMDb_Top_250)

#2: Top_Foreign_Lang_Film = # Write your code to extract top foreign language films from 'IMDb_Top_250' here
Top_Foreign_Lang_Film = IMDb_Top_250[IMDb_Top_250['language'] != 'English']
print(Top_Foreign_Lang_Film)

"""##Checkpoint 3: Can you spot Veer-Zaara in the dataframe?"
"""

if('veer zara is in  Top_Foreign_Lang_Film'):
  print('Veer Zara is among 250 IMdB top 250 movies')
else:
  print('not found')

"""##Subtask 3.5: Find the best directors.

1: Group the dataframe using the director_name column.

2: Find out the top 10 directors for whom the mean of imdb_score is the highest and store them in a new dataframe top10director.
"""

import pandas as pd

# Load the dataset
file_path = "Movie+Assignment+Data.csv"  # Update with the correct file path
df = pd.read_csv(file_path)

# Remove rows with more than 5 NaN values
df_cleaned = df.dropna(thresh=len(df.columns) - 5)

# Remove duplicate rows
df_cleaned_deduplicated = df_cleaned.drop_duplicates()

# Find the top 10 directors with the highest mean IMDb score
top10director = (
    df_cleaned_deduplicated.groupby("director_name")["imdb_score"]
    .mean()
    .nlargest(10)
    .reset_index()
)

# Display the top 10 directors
print(top10director)

"""Checkpoint 4: No surprises that Damien Chazelle (director of Whiplash and La La Land) is in this list.
Answer : Yes Damien Chazelle
is in list.

##Subtask 3.6: Find popular genres

You might have noticed the genres column in the dataframe with all the genres of the movies seperated by a pipe (|). Out of all the movie genres, the first two are most significant for any film.

1: Extract the first two genres from the genres column and store them in two new columns: genre_1 and genre_2. Some of the movies might have only one genre. In such cases, extract the single genre into both the columns, i.e. for such movies the genre_2 will be the same as genre_1.

2: Group the dataframe using genre_1 as the primary column and genre_2 as the secondary column.

3: Find out the 5 most popular combo of genres by finding the mean of the gross values using the gross column and store them in a new dataframe named PopGenre.
"""

movies_by_segment = []# Write your code for grouping the dataframe here"


# Extract the first two genres using str.split and expand=True
movies_by_segment = []# Write your code for grouping the dataframe here"


# Extract the first two genres using str.split and expand=True
#'|', n=1: This part splits the string based on the pipe symbol ('|') with a maximum of 1 split (to get the first two genres).
# expand=True: This creates separate columns for the split results.
movies_cleaned[['genre_1', 'genre_2']] = movies_cleaned['genres'].str.split('|', n=1, expand=True)

# Fill NaN values in genre_2 with genre_1 for movies with only one genre
# fillna(movies_cleaned['genre_1']): It fills any missing values (NaN) in the 'genre_2' column with the corresponding value from the 'genre_1' column.
# This ensures that movies with only one genre have that genre in both 'genre_1' and 'genre_2'.
movies_cleaned['genre_2'] = movies_cleaned['genre_2'].fillna(movies_cleaned['genre_1'])
print(movies_cleaned['genre_2']) # Changed 'genere_2' to 'genre_2'

PopGenre = None # Write your code for getting the 5 most popular combo of genres here


# Group the dataframe by genre_1 and genre_2
movies_by_genre = movies_cleaned.groupby(['genre_1', 'genre_2'])

# Calculate the mean gross for each genre combination, sort, and get the top 5
PopGenre = movies_by_genre['gross'].mean().sort_values(ascending=False).head(5).reset_index()

# Rename columns for clarity
PopGenre.columns = ['genre_1', 'genre_2', 'mean_gross']
# Group the dataframe by genre_1 and genre_2
movies_by_genre = movies_cleaned.groupby(['genre_1', 'genre_2'])

# Calculate the mean gross for each genre combination, sort, and get the top 5
PopGenre = movies_by_genre['gross'].mean().sort_values(ascending=False).head(5).reset_index()

# Rename columns for clarity
PopGenre.columns = ['genre_1', 'genre_2', 'mean_gross']
print(PopGenre) # Changed 'PopGenere' to 'PopGenre'

""""Checkpoint 5: Well, as it turns out. Family + Sci-Fi is the most popular combo of genres out there.

Answer : Yes Family+ Sci-Fi is the most popular combo of genres.

Subtask 3.7: Find the critic-favorite and audience-favorite actors

1: Create three new dataframes namely, Meryl_Streep, Leo_Caprio, and Brad_Pitt which contain the movies in which the actors: 'Meryl Streep', 'Leonardo DiCaprio', and 'Brad Pitt' are the lead actors. Use only the actor_1_name column for extraction. Also, make sure that you use the names 'Meryl Streep', 'Leonardo DiCaprio', and 'Brad Pitt' for the said extraction.

2:Append the rows of all these dataframes and store them in a new dataframe named Combined.

3: Group the combined dataframe using the actor_1_name column.

4: Find the mean of the num_critic_for_reviews and num_user_for_review and identify the actors which have the highest mean.1:
"""

[] # Write your code for creating three new dataframes here

Meryl_Streep = movies_cleaned[movies_cleaned['actor_1_name'] == 'Meryl Streep']
Leo_Caprio = movies_cleaned[movies_cleaned['actor_1_name'] == 'Leonardo DiCaprio']
Brad_Pitt = movies_cleaned[movies_cleaned['actor_1_name'] == 'Brad Pitt']

# Include all movies in which Meryl_Streep is the lead"
 # Include all movies in which Meryl_Streep is the lead
Meryl_Streep = movies_cleaned[movies_cleaned['actor_1_name'] == 'Meryl Streep']
print(Meryl_Streep['movie_title'])  # Access the 'movie_title' column using bracket notation

# Include all movies in which Leo_Caprio is the lead
Leo_Caprio = movies_cleaned[movies_cleaned['actor_1_name'] == 'Leonardo DiCaprio']
print(Leo_Caprio ['movie_title'])  # Access the 'movie_title' column using bracket notation

Brad_Pitt =[] # Include all movies in which Brad_Pitt is the lead
# Include all movies in which Brad_Pitt is the lead
Brad_Pitt = movies_cleaned[movies_cleaned['actor_1_name'] == 'Brad_Pitt ']
print(Brad_Pitt ['movie_title'])  # Access the 'movie_title' column using bracket notation

# Write your code for creating three new dataframes here

Meryl_Streep = movies_cleaned[movies_cleaned['actor_1_name'] == 'Meryl Streep']
Leo_Caprio = movies_cleaned[movies_cleaned['actor_1_name'] == 'Leonardo DiCaprio']
Brad_Pitt = movies_cleaned[movies_cleaned['actor_1_name'] == 'Brad Pitt']
# Write your code for grouping the combined dataframe here
Combined = pd.concat([Meryl_Streep, Leo_Caprio, Brad_Pitt]) #Concatenate the three dataframes
actor_group = Combined.groupby('actor_1_name')  #Group by 'actor_1_name'
print(actor_group)

# Write the code for finding the mean of critic reviews and audience reviews here

 # Write the code for finding the mean of critic reviews and audience reviews here
# Write your code for creating three new dataframes here

Meryl_Streep = movies_cleaned[movies_cleaned['actor_1_name'] == 'Meryl Streep']
Leo_Caprio = movies_cleaned[movies_cleaned['actor_1_name'] == 'Leonardo DiCaprio']
Brad_Pitt = movies_cleaned[movies_cleaned['actor_1_name'] == 'Brad Pitt']

# Write your code for grouping the combined dataframe here
Combined = pd.concat([Meryl_Streep, Leo_Caprio, Brad_Pitt]) #Concatenate the three dataframes
actor_group = Combined.groupby('actor_1_name')  #Group by 'actor_1_name'


# Calculate the mean of critic and audience reviews for each actor
critic_reviews_mean = actor_group['num_critic_for_reviews'].mean()
audience_reviews_mean = actor_group['num_user_for_reviews'].mean()

# Print the results
print("Mean Critic Reviews:")
print(critic_reviews_mean)
print("\nMean Audience Reviews:")
print(audience_reviews_mean)

"""Checkpoint 6: Leonardo has aced both the lists

Yes, Leonardo has aced in both the lists.
"""